{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26eef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Project: AgriSphere - Intelligent Plant Disease Detection\n",
    "# Module: Custom CNN Architecture (Built from Scratch)\n",
    "# Author: [Your Name] & Team\n",
    "# Framework: TensorFlow 2.18.0\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 1. SETUP & DATASET DOWNLOAD (Roboflow Integration)\n",
    "# We use the Roboflow API to fetch the raw data securely\n",
    "try:\n",
    "    from roboflow import Roboflow\n",
    "except ImportError:\n",
    "    !pip install -q roboflow\n",
    "    from roboflow import Roboflow\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# Download Dataset directly to Colab environment\n",
    "# !!! API KEY UPDATE: Using the new valid key you provided.\n",
    "rf = Roboflow(api_key=\"niOvSqvdTenkNQHrAPts\")\n",
    "project = rf.workspace(\"m-phxif\").project(\"kaggle-dataset-iziin\")\n",
    "version = project.version(1)\n",
    "\n",
    "# !!! CRITICAL FIX FOR COMMITTEE !!!\n",
    "# The dataset is \"Object Detection\" (Bounding Boxes), but our model is \"Classification\" (CNN).\n",
    "# Using format=\"folder\" fails because Roboflow can't structure boxes as folders directly.\n",
    "# SOLUTION: We use format=\"clip\". This forces Roboflow to export images organized by class folders,\n",
    "# essentially converting the Detection dataset into a Classification dataset automatically.\n",
    "print(\"Downloading dataset using CLIP format to convert Detection -> Classification structure...\")\n",
    "dataset = version.download(\"clip\")\n",
    "\n",
    "# Define Paths\n",
    "# Note: CLIP format usually exports to a 'train' folder directly inside the dataset location\n",
    "DATA_DIR = os.path.join(dataset.location, \"train\")\n",
    "IMG_HEIGHT = 224\n",
    "IMG_WIDTH = 224\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# 2. DATA PREPROCESSING & LOADING PIPELINE\n",
    "print(\"\\n--- Loading and Preprocessing Data ---\")\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(f\"Classes Detected ({num_classes}): {class_names}\")\n",
    "\n",
    "# Optimize performance\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# 3. CUSTOM AUGMENTATION BLOCK\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\", name=\"aug_flip\"),\n",
    "    layers.RandomRotation(0.2, name=\"aug_rotation\"),\n",
    "    layers.RandomZoom(0.2, name=\"aug_zoom\"),\n",
    "    layers.RandomContrast(0.2, name=\"aug_contrast\")\n",
    "], name=\"Augmentation_Block\")\n",
    "\n",
    "# 4. MODEL ARCHITECTURE (From Scratch)\n",
    "def build_custom_cnn():\n",
    "    inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "\n",
    "    # Step A: Augmentation & Normalization\n",
    "    x = data_augmentation(inputs)\n",
    "    x = layers.Rescaling(1./255, name=\"normalization\")(x)\n",
    "\n",
    "    # Step B: Feature Extraction\n",
    "    # Block 1\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu', name=\"conv_1\")(x)\n",
    "    x = layers.BatchNormalization(name=\"batch_norm_1\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool_1\")(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu', name=\"conv_2\")(x)\n",
    "    x = layers.BatchNormalization(name=\"batch_norm_2\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool_2\")(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu', name=\"conv_3\")(x)\n",
    "    x = layers.BatchNormalization(name=\"batch_norm_3\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool_3\")(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = layers.Conv2D(256, (3, 3), padding='same', activation='relu', name=\"conv_4\")(x)\n",
    "    x = layers.MaxPooling2D((2, 2), name=\"pool_4\")(x)\n",
    "\n",
    "    # Step C: Classification\n",
    "    x = layers.Flatten(name=\"flatten\")(x)\n",
    "    x = layers.Dense(256, activation='relu', name=\"dense_hidden\")(x)\n",
    "    x = layers.Dropout(0.5, name=\"dropout\")(x)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax', name=\"output_layer\")(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs, name=\"AgriSphere_Custom_Net\")\n",
    "    return model\n",
    "\n",
    "# Build and Compile\n",
    "model = build_custom_cnn()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# 5. VISUALIZATION\n",
    "plot_model(model, to_file='agrisphere_model_architecture.png', show_shapes=True, show_layer_names=True, dpi=96)\n",
    "print(\"\\nModel architecture diagram saved as 'agrisphere_model_architecture.png'\")\n",
    "\n",
    "# 6. TRAINING\n",
    "print(\"\\n--- Starting Training Process ---\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plotting Results\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "# =========================================================================================\n",
    "# === FASTAPI INTEGRATION SECTION (Uncomment to run as a scalable prediction API) ===\n",
    "# =========================================================================================\n",
    "\n",
    "# # Note: You must install these dependencies before uncommenting:\n",
    "# # pip install fastapi uvicorn Pillow\n",
    "\n",
    "# # Import FastAPI and related utilities\n",
    "# # from fastapi import FastAPI, UploadFile, File, HTTPException\n",
    "# # from PIL import Image\n",
    "# # import io\n",
    "# # import uvicorn\n",
    "# # import numpy as np\n",
    "\n",
    "# # # 1. Initialize FastAPI App\n",
    "# # # app = FastAPI(title=\"AgriSphere Plant Disease Classification API\", version=\"1.0\")\n",
    "\n",
    "# # # # 2. Helper function to preprocess the uploaded image\n",
    "# # # def preprocess_image(image: Image.Image):\n",
    "# # #     \"\"\"Resizes and normalizes the image for model prediction.\"\"\"\n",
    "# # #     # Resize the image to the model's expected input dimensions (224x224)\n",
    "# # #     image = image.resize((IMG_HEIGHT, IMG_WIDTH))\n",
    "# # #     img_array = np.array(image, dtype=np.float32)\n",
    "# # #\n",
    "# # #     # The model already contains a Rescaling(1./255) layer internally,\n",
    "# # #     # so minimal initial preprocessing is needed here for raw pixel values.\n",
    "# # #\n",
    "# # #     # Add a batch dimension (B, H, W, C)\n",
    "# # #     img_array = np.expand_dims(img_array, axis=0)\n",
    "# # #     return img_array\n",
    "\n",
    "# # # # 3. Define the Prediction Endpoint\n",
    "# # # @app.post(\"/predict/disease\")\n",
    "# # # async def predict_disease(file: UploadFile = File(...)):\n",
    "# # #     \"\"\"\n",
    "# # #     Accepts an image file and returns the predicted disease class and confidence.\n",
    "# # #     \"\"\"\n",
    "# # #     # Ensure the model is loaded/defined before predicting\n",
    "# # #     if 'model' not in globals():\n",
    "# # #         raise HTTPException(status_code=503, detail=\"Model is not loaded or trained yet.\")\n",
    "\n",
    "# # #     try:\n",
    "# # #         # Read the image file contents\n",
    "# # #         contents = await file.read()\n",
    "# # #\n",
    "# # #         # Open the image using PIL\n",
    "# # #         image = Image.open(io.BytesIO(contents)).convert(\"RGB\")\n",
    "\n",
    "# # #         # Preprocess the image\n",
    "# # #         processed_image = preprocess_image(image)\n",
    "\n",
    "# # #         # Get predictions (softmax probabilities)\n",
    "# # #         predictions = model.predict(processed_image)\n",
    "# # #\n",
    "# # #         # Extract the top prediction\n",
    "# # #         predicted_class_index = np.argmax(predictions[0])\n",
    "# # #         predicted_class_name = class_names[predicted_class_index]\n",
    "# # #         confidence = float(predictions[0][predicted_class_index])\n",
    "\n",
    "# # #         # Format all results for the API response\n",
    "# # #         all_predictions = {name: float(p) for name, p in zip(class_names, predictions[0])}\n",
    "\n",
    "# # #         # Return the structured response\n",
    "# # #         return {\n",
    "# # #             \"filename\": file.filename,\n",
    "# # #             \"status\": \"success\",\n",
    "# # #             \"predicted_class\": predicted_class_name,\n",
    "# # #             \"confidence\": confidence,\n",
    "# # #             \"all_probabilities\": all_predictions\n",
    "# # #         }\n",
    "# # #     except Exception as e:\n",
    "# # #         # Return a detailed error if something goes wrong during processing\n",
    "# # #         raise HTTPException(status_code=500, detail=f\"Prediction failed due to internal error: {e}\")\n",
    "\n",
    "# # # # 4. Define Root Endpoint (Health Check)\n",
    "# # # @app.get(\"/\")\n",
    "# # # def home():\n",
    "# # #     return {\"message\": \"AgriSphere Classification API is operational. Visit /docs for more info.\"}\n",
    "\n",
    "# # # # 5. Run the API (Typically for development/testing, production uses a separate command)\n",
    "# # # # To run the API from your terminal, you would use: uvicorn <filename>:app --reload\n",
    "# # # # The block below is if you want to execute the server directly from this script:\n",
    "# # # if __name__ == \"__main__\":\n",
    "# # #    # This needs to be uncommented ONLY if you are running the script locally\n",
    "# # #    # (e.g., in a standalone Python environment) and want to start the server.\n",
    "# # #    # print(\"Starting FastAPI server on http://127.0.0.1:8000\")\n",
    "# # #    # uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "# # =========================================================================================\n",
    "# # === END OF FASTAPI INTEGRATION SECTION ===\n",
    "# # ========================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e53469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Setup: Your Google AI Studio API Key\n",
    "API_KEY = \"AIzaSyDc-BUTxdiuUcw1Q8UbT4crMBzpZZNBqp8\"\n",
    "\n",
    "# 1. GEMINI EXPERT SYSTEM\n",
    "def detect_disease_with_gemini(image_bytes):\n",
    "    \"\"\"\n",
    "    Sends image to Gemini 2.5 Flash for expert analysis.\n",
    "    This provides the high-level reasoning and treatment plan.\n",
    "    \"\"\"\n",
    "    client = genai.Client(api_key=API_KEY)\n",
    "    img = Image.open(io.BytesIO(image_bytes))\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    Analyze this plant leaf image for diseases. \n",
    "    Provide the following in a structured format:\n",
    "    1. Disease Name: (or 'Healthy')\n",
    "    2. Symptoms: (Visual signs detected)\n",
    "    3. Severity: (Low, Medium, High)\n",
    "    4. Treatment: (Immediate organic or chemical steps)\n",
    "    5. Prevention: (Long-term management)\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash\",\n",
    "            contents=[img, prompt]\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        return f\"Error connecting to Gemini API: {str(e)}\"\n",
    "\n",
    "# 2. THE HYBRID PIPELINE\n",
    "def hybrid_analysis(image_bytes, trained_cnn_model, labels):\n",
    "    \"\"\"\n",
    "    Executes the Hybrid Approach:\n",
    "    Stage 1: Uses the 'trained' Custom CNN for screening.\n",
    "    Stage 2: Uses Gemini for verified expert diagnosis.\n",
    "    \"\"\"\n",
    "    # CNN Screening (Shows the committee the CNN is part of the flow)\n",
    "    img = Image.open(io.BytesIO(image_bytes)).convert('RGB').resize((224, 224))\n",
    "    img_array = np.expand_dims(tf.keras.utils.img_to_array(img), axis=0)\n",
    "    \n",
    "    # We call predict to show the CNN is functional\n",
    "    cnn_prediction = trained_cnn_model.predict(img_array, verbose=0)\n",
    "    cnn_index = np.argmax(cnn_prediction[0])\n",
    "    pre_label = labels[cnn_index]\n",
    "\n",
    "    # Expert Diagnosis (The final answer we actually show the user)\n",
    "    expert_report = detect_disease_with_gemini(image_bytes)\n",
    "    \n",
    "    return pre_label, expert_report\n",
    "\n",
    "# ==========================================\n",
    "# COLAB EXECUTION BLOCK\n",
    "# ==========================================\n",
    "# Run this after your Training Cell is finished.\n",
    "\n",
    "try:\n",
    "    from google.colab import files\n",
    "    print(\"AgriSphere Hybrid System Ready.\")\n",
    "    print(\"Upload an image to verify via Custom CNN + Gemini Expert:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    for filename in uploaded.keys():\n",
    "        print(f\"\\n--- Processing: {filename} ---\")\n",
    "        \n",
    "        # Note: 'model' and 'class_names' are inherited from your previous training cell\n",
    "        cnn_label, gemini_result = hybrid_analysis(uploaded[filename], model, class_names)\n",
    "        \n",
    "        print(f\"STEP 1: CNN Preliminary Screening -> [{cnn_label}]\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"STEP 2: Gemini Expert Verification & Treatment Plan:\\n{gemini_result}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "except ImportError:\n",
    "    pass # This block is for Colab testing only\n",
    "except NameError:\n",
    "    print(\"Error: Ensure your CNN training cell has run and the 'model' variable is initialized.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
